{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé¨ YOWO Multi-Task Training on Google Colab\n",
        "\n",
        "**Model**: `yowo_v2_x3d_m_yolo11m_multitask`  \n",
        "**Dataset**: Charades + Action Genome (288K keyframes, 219 classes)\n",
        "\n",
        "### ‚ú® New Improvements (Dec 2024)\n",
        "- **Soft-Argmax**: Differentiable position extraction for location-aware context\n",
        "- **Action-Object Co-occurrence**: Learns which objects predict which actions\n",
        "- **Learnable Temporal Attention**: Task-specific timestep weighting in X3D\n",
        "- **Label Smoothing**: Optional regularization for rare classes\n",
        "- **All Backbones Trainable**: YOLO11 + X3D fully fine-tuned (26.9M params)\n",
        "\n",
        "### Optimized Batch Sizes (with AMP)\n",
        "\n",
        "| GPU | VRAM | Batch | Accum | Effective | Est. Time/Epoch |\n",
        "|-----|------|-------|-------|-----------|-----------------||\n",
        "| T4 | 16GB | 8 | 4 | 32 | ~2.5 hours |\n",
        "| L4 | 24GB | 14 | 4 | 56 | ~1.5 hours |\n",
        "| V100 | 16GB | 10 | 4 | 40 | ~1.5 hours |\n",
        "| A100 | 40GB | 28 | 2 | 56 | ~45 min |\n",
        "| A100 | 80GB | 56 | 2 | 112 | ~25 min |\n",
        "| H100 | 80GB | 80 | 2 | 160 | ~15 min |\n",
        "\n",
        "**Features**: AMP (FP16), Multi-head (Objects + Actions + Relationships)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Check GPU & Auto-Configure Batch Size\n",
        "import torch\n",
        "print(\"=\" * 70)\n",
        "print(\"üîç GPU Detection & Configuration\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ùå No GPU! Go to Runtime > Change runtime type > GPU\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "print(f\"‚úÖ VRAM: {gpu_memory_gb:.1f} GB\")\n",
        "\n",
        "# =============================================================================\n",
        "# OPTIMIZED BATCH SIZES FOR YOWO V2 + X3D-M + YOLO11m WITH AMP\n",
        "# Based on empirical testing of video action detection models\n",
        "# AMP reduces memory by ~40%, allowing larger batches\n",
        "# =============================================================================\n",
        "if \"A100\" in gpu_name or \"A100\" in gpu_name.upper():\n",
        "    if gpu_memory_gb > 45:  # A100 80GB\n",
        "        BATCH_SIZE, ACCUMULATE = 64, 2   # Effective: 128 (can try 80 if stable)\n",
        "    else:  # A100 40GB\n",
        "        BATCH_SIZE, ACCUMULATE = 32, 2   # Effective: 64 (can try 40-48)\n",
        "elif \"H100\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 80, 2       # Effective: 160 (can try 96)\n",
        "elif \"L4\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 12, 4       # Effective: 48\n",
        "elif \"T4\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 8, 4        # Effective: 32 (can try 10)\n",
        "elif \"V100\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 10, 4       # Effective: 40\n",
        "elif \"P100\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 6, 4        # Effective: 24\n",
        "else:\n",
        "    # Unknown GPU - use conservative settings based on memory\n",
        "    if gpu_memory_gb >= 40:\n",
        "        BATCH_SIZE, ACCUMULATE = 32, 2\n",
        "    elif gpu_memory_gb >= 20:\n",
        "        BATCH_SIZE, ACCUMULATE = 12, 4\n",
        "    else:\n",
        "        BATCH_SIZE, ACCUMULATE = 8, 4\n",
        "\n",
        "effective = BATCH_SIZE * ACCUMULATE\n",
        "print(f\"\\nüì¶ Optimized for {gpu_name}:\")\n",
        "print(f\"   batch_size = {BATCH_SIZE}\")\n",
        "print(f\"   accumulate = {ACCUMULATE}\")\n",
        "print(f\"   effective_batch = {effective}\")\n",
        "print(f\"\\nüí° If OOM: reduce BATCH_SIZE by 2, increase ACCUMULATE proportionally\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Clone Repository & Install Dependencies\n",
        "%cd /content\n",
        "!rm -rf yowo\n",
        "!git clone https://github.com/michelsedgh/yowo.git\n",
        "%cd yowo\n",
        "!pip install -q torch torchvision opencv-python thop scipy matplotlib numpy imageio pytorchvideo ultralytics tensorboard\n",
        "print(\"‚úÖ Repository cloned and dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Download Annotations & Extract Frames\n",
        "import os, time, requests, zipfile\n",
        "\n",
        "DATA_ROOT = \"/content/yowo/data/ActionGenome\"\n",
        "FRAMES_DIR = os.path.join(DATA_ROOT, \"frames\")\n",
        "ANN_DIR = os.path.join(DATA_ROOT, \"annotations\")\n",
        "TAR_PATH = \"/content/drive/MyDrive/yooowo/frames.tar\"\n",
        "\n",
        "os.makedirs(ANN_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Download Action Genome annotations (PKL files NOT in git repo!)\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"üì• STEP 1: Downloading Action Genome Annotations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def download_file(url, filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        size = os.path.getsize(filepath) / 1e6\n",
        "        print(f\"   ‚úÖ {os.path.basename(filepath)} exists ({size:.1f} MB)\")\n",
        "        return True\n",
        "    print(f\"   Downloading {os.path.basename(filepath)}...\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, timeout=120)\n",
        "        if response.status_code == 200:\n",
        "            with open(filepath, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            size = os.path.getsize(filepath) / 1e6\n",
        "            print(f\"   ‚úÖ Downloaded ({size:.1f} MB)\")\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed: {e}\")\n",
        "    return False\n",
        "\n",
        "# Action Genome annotations from STAR Benchmark S3\n",
        "ag_files = {\n",
        "    'object_bbox_and_relationship.pkl': 'https://star-benchmark.s3.us-east.cloud-object-storage.appdomain.cloud/Annotations/object_bbox_and_relationship.pkl',\n",
        "    'person_bbox.pkl': 'https://star-benchmark.s3.us-east.cloud-object-storage.appdomain.cloud/Annotations/person_bbox.pkl',\n",
        "    'classes.zip': 'https://star-benchmark.s3.us-east.cloud-object-storage.appdomain.cloud/Annotations/classes.zip'\n",
        "}\n",
        "\n",
        "for filename, url in ag_files.items():\n",
        "    download_file(url, os.path.join(ANN_DIR, filename))\n",
        "\n",
        "# Extract classes.zip if needed\n",
        "classes_zip = os.path.join(ANN_DIR, 'classes.zip')\n",
        "if os.path.exists(classes_zip) and not os.path.exists(os.path.join(ANN_DIR, 'object_classes.txt')):\n",
        "    print(\"   Extracting classes.zip...\")\n",
        "    with zipfile.ZipFile(classes_zip, 'r') as z:\n",
        "        z.extractall(ANN_DIR)\n",
        "    # Move files from classes/ subdirectory if needed\n",
        "    classes_subdir = os.path.join(ANN_DIR, 'classes')\n",
        "    if os.path.exists(classes_subdir):\n",
        "        import shutil\n",
        "        for f in os.listdir(classes_subdir):\n",
        "            shutil.move(os.path.join(classes_subdir, f), os.path.join(ANN_DIR, f))\n",
        "        shutil.rmtree(classes_subdir)\n",
        "    print(\"   ‚úÖ Extracted class files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2 import credentials\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "# 1. The Main Archive\n",
        "TAR_FILE_ID = \"1GuRdUMP5qrqyYN0gg8C2B6tLwJeigyFd\"  \n",
        "LOCAL_TAR = \"/content/frames.tar\"\n",
        "\n",
        "# 2. The Pre-made Index (To save time!)\n",
        "INDEX_FILE_ID = \"1ecTAlWCWWSfSavneBwlALjhocl3LKXoa\"\n",
        "LOCAL_INDEX = \"/content/frames.tar.index.sqlite\"\n",
        "\n",
        "# 3. Paths\n",
        "# We mount the raw tar here first\n",
        "TEMP_MOUNT_POINT = \"/content/raw_mount\" \n",
        "# We want the data to appear here eventually\n",
        "FINAL_TARGET_DIR = \"/content/yowo/data/ActionGenome/frames\"\n",
        "# ==============================================================================\n",
        "\n",
        "def install_tools():\n",
        "    print(\"üõ†Ô∏è Installing aria2 and ratarmount...\")\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"-qq\", \"aria2\"], check=True)\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"ratarmount\"], check=True)\n",
        "\n",
        "def get_token():\n",
        "    print(\"üîë Authenticating...\")\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = google.auth.default()\n",
        "    creds.refresh(Request())\n",
        "    return creds.token\n",
        "\n",
        "def download_file(token, file_id, output_path):\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"‚úÖ Found existing file: {output_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"‚¨áÔ∏è Downloading {os.path.basename(output_path)}...\")\n",
        "    url = f\"https://www.googleapis.com/drive/v3/files/{file_id}?alt=media\"\n",
        "    \n",
        "    cmd = [\n",
        "        \"aria2c\", \"-x\", \"16\", \"-s\", \"16\", \"-j\", \"16\",\n",
        "        \"--file-allocation=none\", \"--summary-interval=10\",\n",
        "        \"--header\", f\"Authorization: Bearer {token}\", \n",
        "        \"-o\", os.path.basename(output_path),\n",
        "        \"-d\", os.path.dirname(output_path),\n",
        "        url\n",
        "    ]\n",
        "    \n",
        "    process = subprocess.Popen(cmd)\n",
        "    process.wait()\n",
        "    \n",
        "    if process.returncode != 0:\n",
        "        raise Exception(f\"Failed to download {output_path}\")\n",
        "\n",
        "def mount_and_link():\n",
        "    print(f\"\\nüîó Mounting archive to temp location: {TEMP_MOUNT_POINT}\")\n",
        "    \n",
        "    # 1. Cleanup\n",
        "    subprocess.run([\"fusermount\", \"-u\", TEMP_MOUNT_POINT], stderr=subprocess.DEVNULL)\n",
        "    if os.path.islink(FINAL_TARGET_DIR):\n",
        "        os.unlink(FINAL_TARGET_DIR)\n",
        "    elif os.path.exists(FINAL_TARGET_DIR):\n",
        "        # If it's an empty dir, remove it so we can link\n",
        "        try: os.rmdir(FINAL_TARGET_DIR)\n",
        "        except: pass\n",
        "\n",
        "    os.makedirs(TEMP_MOUNT_POINT, exist_ok=True)\n",
        "    \n",
        "    # 2. Ratarmount using the downloaded index\n",
        "    # We pass the index file explicitly\n",
        "    cmd = f\"ratarmount -P 4 --index-file '{LOCAL_INDEX}' '{LOCAL_TAR}' '{TEMP_MOUNT_POINT}'\"\n",
        "    exit_code = os.system(cmd)\n",
        "    \n",
        "    if exit_code != 0:\n",
        "        raise Exception(\"Ratarmount failed!\")\n",
        "\n",
        "    # 3. Find the internal data path and Link it\n",
        "    # Based on your error, the data is nested inside:\n",
        "    nested_path = os.path.join(TEMP_MOUNT_POINT, \"data/ActionGenome/frames\")\n",
        "    \n",
        "    # Fallback: If that exact path doesn't exist, list folders to help debug\n",
        "    if not os.path.exists(nested_path):\n",
        "        print(f\"‚ö†Ô∏è Could not find expected path: {nested_path}\")\n",
        "        print(f\"üìÇ Contents of root mount: {os.listdir(TEMP_MOUNT_POINT)}\")\n",
        "        # Try to find 'frames' folder dynamically?\n",
        "        # For now, let's assume the structure you mentioned is correct.\n",
        "    \n",
        "    # 4. Create the final destination link\n",
        "    # Ensure parent dir exists\n",
        "    parent_dir = os.path.dirname(FINAL_TARGET_DIR)\n",
        "    os.makedirs(parent_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"üîó Linking '{nested_path}' --> '{FINAL_TARGET_DIR}'\")\n",
        "    os.symlink(nested_path, FINAL_TARGET_DIR)\n",
        "    \n",
        "    # 5. Verify\n",
        "    if os.path.exists(FINAL_TARGET_DIR) and len(os.listdir(FINAL_TARGET_DIR)) > 0:\n",
        "        count = len(os.listdir(FINAL_TARGET_DIR))\n",
        "        print(f\"üéâ SUCCESS! {count} items visible at {FINAL_TARGET_DIR}\")\n",
        "    else:\n",
        "        print(\"‚ùå Something went wrong. The target folder is empty.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "try:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.flush_and_unmount()\n",
        "    except: pass\n",
        "    \n",
        "    install_tools()\n",
        "    token = get_token()\n",
        "    \n",
        "    # Download Tar AND Index\n",
        "    download_file(token, TAR_FILE_ID, LOCAL_TAR)\n",
        "    download_file(token, INDEX_FILE_ID, LOCAL_INDEX)\n",
        "    \n",
        "    mount_and_link()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå CRITICAL ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Verify Dataset Structure\n",
        "import os, pickle\n",
        "\n",
        "ANN_DIR = \"/content/yowo/data/ActionGenome/annotations\"\n",
        "FRAMES_DIR = \"/content/yowo/data/ActionGenome/frames\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîç Dataset Verification\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check required files\n",
        "required_files = {\n",
        "    'person_bbox.pkl': 'Person bounding boxes + keyframes',\n",
        "    'object_bbox_and_relationship.pkl': 'Objects + relationships',\n",
        "    'Charades_v1_train.csv': 'Training action labels',\n",
        "    'Charades_v1_test.csv': 'Test action labels',\n",
        "    'Charades_v1_classes.txt': '157 action classes',\n",
        "    'object_classes.txt': '36 object classes',\n",
        "    'relationship_classes.txt': '26 relationship classes',\n",
        "    'video_fps.json': 'FPS for each video'\n",
        "}\n",
        "\n",
        "print(\"\\nüìã Required Annotation Files:\")\n",
        "all_ok = True\n",
        "for f, desc in required_files.items():\n",
        "    path = os.path.join(ANN_DIR, f)\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / 1e6\n",
        "        print(f\"   ‚úÖ {f} ({size:.1f} MB) - {desc}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {f} - MISSING! ({desc})\")\n",
        "        all_ok = False\n",
        "\n",
        "# Check frames\n",
        "print(f\"\\nüìÇ Frames Directory:\")\n",
        "if os.path.exists(FRAMES_DIR):\n",
        "    num_videos = len(os.listdir(FRAMES_DIR))\n",
        "    print(f\"   ‚úÖ {num_videos} video directories\")\n",
        "    # Sample a video\n",
        "    sample_vid = os.listdir(FRAMES_DIR)[0]\n",
        "    sample_frames = len(os.listdir(os.path.join(FRAMES_DIR, sample_vid)))\n",
        "    print(f\"   üìÅ Sample: {sample_vid} has {sample_frames} frames\")\n",
        "else:\n",
        "    print(\"   ‚ùå Frames directory missing!\")\n",
        "    all_ok = False\n",
        "\n",
        "# Verify PKL files are valid\n",
        "print(f\"\\nüî¨ Validating PKL Files:\")\n",
        "try:\n",
        "    with open(os.path.join(ANN_DIR, 'person_bbox.pkl'), 'rb') as f:\n",
        "        person_data = pickle.load(f)\n",
        "    print(f\"   ‚úÖ person_bbox.pkl: {len(person_data)} keyframes\")\n",
        "    \n",
        "    with open(os.path.join(ANN_DIR, 'object_bbox_and_relationship.pkl'), 'rb') as f:\n",
        "        obj_data = pickle.load(f)\n",
        "    print(f\"   ‚úÖ object_bbox_and_relationship.pkl: {len(obj_data)} entries\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error reading PKL files: {e}\")\n",
        "    all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ DATASET READY FOR TRAINING!\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚ö†Ô∏è DATASET INCOMPLETE - Check errors above\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Ready to Train!\n",
        "\n",
        "**Model Architecture: `yowo_v2_x3d_m_yolo11m_multitask`**\n",
        "\n",
        "| Component | Description |\n",
        "|-----------|-------------|\n",
        "| 2D Backbone | YOLO11m (pretrained on COCO, **TRAINABLE**) |\n",
        "| 3D Backbone | X3D-M with **Learnable Temporal Attention** |\n",
        "| Object Head | 36 classes (person + 35 objects) |\n",
        "| Action Head | 157 Charades classes + **Action-Object Co-occurrence** |\n",
        "| Relation Head | 26 relationship classes |\n",
        "| Context | **Soft-Argmax** for differentiable positions |\n",
        "\n",
        "**New Features (Dec 2024)**\n",
        "- ‚úÖ All 26.9M params trainable (backbones unfrozen by default)\n",
        "- ‚úÖ Soft-argmax for gradient flow through positions\n",
        "- ‚úÖ Action-Object co-occurrence learning\n",
        "- ‚úÖ Learnable temporal attention in X3D\n",
        "- ‚úÖ Optional label smoothing\n",
        "\n",
        "**Dataset: Charades + Action Genome**\n",
        "- 288,782 annotated keyframes\n",
        "- 9,601 videos\n",
        "- Multi-task: Objects + Actions + Relationships\n",
        "\n",
        "**Note:** Model checkpoints saved after each epoch to `/content/yowo/weights/charades_ag/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# üöÄ IMPROVED TRAINING CELL (Respects GPU Auto-Detection)\n",
        "# =============================================================================\n",
        "import os\n",
        "os.chdir('/content/yowo')\n",
        "\n",
        "# CONFIGURABLE PARAMETERS\n",
        "# -----------------------------------------------------------------------------\n",
        "# Use auto-detected batch size from Cell 1 if it exists, otherwise use safe defaults\n",
        "try:\n",
        "    current_bs = BATCH_SIZE\n",
        "    current_acc = ACCUMULATE\n",
        "    print(f\"‚úÖ Using detected batch size: {current_bs} (accumulate={current_acc})\")\n",
        "except NameError:\n",
        "    current_bs = 8\n",
        "    current_acc = 4\n",
        "    print(f\"‚ö†Ô∏è GPU Detection not found, using conservative defaults: {current_bs}\")\n",
        "\n",
        "MAX_EPOCHS = 15\n",
        "LEN_CLIP = 16\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "# Learning Rate: Heads get this, Backbones get 1/10th of this (0.00002)\n",
        "LEARNING_RATE = 0.0003    \n",
        "LR_DECAY_EPOCHS = \"8 10 12\"\n",
        "LABEL_SMOOTHING = 0.05    \n",
        "\n",
        "# Backbone Training\n",
        "FREEZE_2D = False         \n",
        "FREEZE_3D = False         \n",
        "\n",
        "# RESUME: Set to path or None\n",
        "RESUME_CHECKPOINT = None\n",
        "\n",
        "# BUILD COMMAND\n",
        "# -----------------------------------------------------------------------------\n",
        "cmd = f\"\"\"python train.py \\\n",
        "    -d charades_ag \\\n",
        "    -v yowo_v2_x3d_m_yolo11m_multitask \\\n",
        "    --cuda \\\n",
        "    --amp \\\n",
        "    --eval \\\n",
        "    -bs {current_bs} \\\n",
        "    -accu {current_acc} \\\n",
        "    --max_epoch {MAX_EPOCHS} \\\n",
        "    --lr_epoch {LR_DECAY_EPOCHS} \\\n",
        "    --root /content/yowo/data \\\n",
        "    -K {LEN_CLIP} \\\n",
        "    -lr {LEARNING_RATE} \\\n",
        "    --label_smoothing {LABEL_SMOOTHING} \\\n",
        "    --num_workers {NUM_WORKERS} \\\n",
        "    --save_folder /content/yowo/weights\"\"\"\n",
        "\n",
        "if FREEZE_2D: cmd += \" --freeze_backbone_2d\"\n",
        "if FREEZE_3D: cmd += \" --freeze_backbone_3d\"\n",
        "if RESUME_CHECKPOINT: cmd += f\" -r {RESUME_CHECKPOINT}\"\n",
        "\n",
        "# Trainable params helper (YOLO11m is ~20M, X3D-M is ~3M, Heads are ~4M)\n",
        "trainable_m = 26.9\n",
        "if FREEZE_2D: trainable_m -= 20.1\n",
        "if FREEZE_3D: trainable_m -= 3.0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(f\"üöÄ TRAINING CONFIGURATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üì¶ Batch: {current_bs} √ó {current_acc} = {current_bs * current_acc} effective\")\n",
        "print(f\"üìà LR: {LEARNING_RATE} (Heads) / {LEARNING_RATE*0.1:.6f} (Backbones)\")\n",
        "print(f\"üß† Total Trainable Params: ~{trainable_m:.1f}M\")\n",
        "print(f\"üéØ Features: Soft-Argmax, Co-occurrence, Temporal Attention\")\n",
        "print(f\"\\nüìã Full Command:\\n{cmd}\\n\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "!{cmd}"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
