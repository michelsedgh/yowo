{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé¨ YOWO Multi-Task Training on Google Colab\n",
        "\n",
        "**Model**: `yowo_v2_x3d_m_yolo11m_multitask`  \n",
        "**Dataset**: Charades + Action Genome (288K keyframes, 219 classes)\n",
        "\n",
        "### Optimized Batch Sizes (with AMP)\n",
        "\n",
        "| GPU | VRAM | Batch | Accum | Effective | Est. Time/Epoch |\n",
        "|-----|------|-------|-------|-----------|-----------------|\n",
        "| T4 | 16GB | 8 | 4 | 32 | ~4 hours |\n",
        "| L4 | 24GB | 12 | 4 | 48 | ~2.5 hours |\n",
        "| V100 | 16GB | 10 | 4 | 40 | ~2 hours |\n",
        "| A100 | 40GB | 32 | 2 | 64 | ~50 min |\n",
        "| A100 | 80GB | 64 | 2 | 128 | ~30 min |\n",
        "| H100 | 80GB | 80 | 2 | 160 | ~20 min |\n",
        "\n",
        "**Features**: AMP (FP16), Multi-head (Objects + Actions + Relationships)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Check GPU & Auto-Configure Batch Size\n",
        "import torch\n",
        "print(\"=\" * 70)\n",
        "print(\"üîç GPU Detection & Configuration\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ùå No GPU! Go to Runtime > Change runtime type > GPU\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "print(f\"‚úÖ VRAM: {gpu_memory_gb:.1f} GB\")\n",
        "\n",
        "# =============================================================================\n",
        "# OPTIMIZED BATCH SIZES FOR YOWO V2 + X3D-M + YOLO11m WITH AMP\n",
        "# Based on empirical testing of video action detection models\n",
        "# AMP reduces memory by ~40%, allowing larger batches\n",
        "# =============================================================================\n",
        "if \"A100\" in gpu_name or \"A100\" in gpu_name.upper():\n",
        "    if gpu_memory_gb > 45:  # A100 80GB\n",
        "        BATCH_SIZE, ACCUMULATE = 64, 2   # Effective: 128 (can try 80 if stable)\n",
        "    else:  # A100 40GB\n",
        "        BATCH_SIZE, ACCUMULATE = 32, 2   # Effective: 64 (can try 40-48)\n",
        "elif \"H100\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 80, 2       # Effective: 160 (can try 96)\n",
        "elif \"L4\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 12, 4       # Effective: 48\n",
        "elif \"T4\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 8, 4        # Effective: 32 (can try 10)\n",
        "elif \"V100\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 10, 4       # Effective: 40\n",
        "elif \"P100\" in gpu_name:\n",
        "    BATCH_SIZE, ACCUMULATE = 6, 4        # Effective: 24\n",
        "else:\n",
        "    # Unknown GPU - use conservative settings based on memory\n",
        "    if gpu_memory_gb >= 40:\n",
        "        BATCH_SIZE, ACCUMULATE = 32, 2\n",
        "    elif gpu_memory_gb >= 20:\n",
        "        BATCH_SIZE, ACCUMULATE = 12, 4\n",
        "    else:\n",
        "        BATCH_SIZE, ACCUMULATE = 8, 4\n",
        "\n",
        "effective = BATCH_SIZE * ACCUMULATE\n",
        "print(f\"\\nüì¶ Optimized for {gpu_name}:\")\n",
        "print(f\"   batch_size = {BATCH_SIZE}\")\n",
        "print(f\"   accumulate = {ACCUMULATE}\")\n",
        "print(f\"   effective_batch = {effective}\")\n",
        "print(f\"\\nüí° If OOM: reduce BATCH_SIZE by 2, increase ACCUMULATE proportionally\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "TAR_PATH = \"/content/drive/MyDrive/yooowo/frames.tar\"\n",
        "if os.path.exists(TAR_PATH):\n",
        "    size_gb = os.path.getsize(TAR_PATH) / 1e9\n",
        "    print(f\"‚úÖ Found frames.tar ({size_gb:.2f} GB)\")\n",
        "else:\n",
        "    print(f\"‚ùå frames.tar not found at {TAR_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Clone Repository & Install Dependencies\n",
        "%cd /content\n",
        "!rm -rf yowo\n",
        "!git clone https://github.com/michelsedgh/yowo.git\n",
        "%cd yowo\n",
        "!pip install -q torch torchvision opencv-python thop scipy matplotlib numpy imageio pytorchvideo ultralytics tensorboard\n",
        "print(\"‚úÖ Repository cloned and dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Download Annotations & Extract Frames\n",
        "import os, time, requests, zipfile\n",
        "\n",
        "DATA_ROOT = \"/content/yowo/data/ActionGenome\"\n",
        "FRAMES_DIR = os.path.join(DATA_ROOT, \"frames\")\n",
        "ANN_DIR = os.path.join(DATA_ROOT, \"annotations\")\n",
        "TAR_PATH = \"/content/drive/MyDrive/yooowo/frames.tar\"\n",
        "\n",
        "os.makedirs(ANN_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Download Action Genome annotations (PKL files NOT in git repo!)\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"üì• STEP 1: Downloading Action Genome Annotations\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def download_file(url, filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        size = os.path.getsize(filepath) / 1e6\n",
        "        print(f\"   ‚úÖ {os.path.basename(filepath)} exists ({size:.1f} MB)\")\n",
        "        return True\n",
        "    print(f\"   Downloading {os.path.basename(filepath)}...\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True, timeout=120)\n",
        "        if response.status_code == 200:\n",
        "            with open(filepath, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            size = os.path.getsize(filepath) / 1e6\n",
        "            print(f\"   ‚úÖ Downloaded ({size:.1f} MB)\")\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Failed: {e}\")\n",
        "    return False\n",
        "\n",
        "# Action Genome annotations from STAR Benchmark S3\n",
        "ag_files = {\n",
        "    'object_bbox_and_relationship.pkl': 'https://star-benchmark.s3.us-east.cloud-object-storage.appdomain.cloud/Annotations/object_bbox_and_relationship.pkl',\n",
        "    'person_bbox.pkl': 'https://star-benchmark.s3.us-east.cloud-object-storage.appdomain.cloud/Annotations/person_bbox.pkl',\n",
        "    'classes.zip': 'https://star-benchmark.s3.us-east.cloud-object-storage.appdomain.cloud/Annotations/classes.zip'\n",
        "}\n",
        "\n",
        "for filename, url in ag_files.items():\n",
        "    download_file(url, os.path.join(ANN_DIR, filename))\n",
        "\n",
        "# Extract classes.zip if needed\n",
        "classes_zip = os.path.join(ANN_DIR, 'classes.zip')\n",
        "if os.path.exists(classes_zip) and not os.path.exists(os.path.join(ANN_DIR, 'object_classes.txt')):\n",
        "    print(\"   Extracting classes.zip...\")\n",
        "    with zipfile.ZipFile(classes_zip, 'r') as z:\n",
        "        z.extractall(ANN_DIR)\n",
        "    # Move files from classes/ subdirectory if needed\n",
        "    classes_subdir = os.path.join(ANN_DIR, 'classes')\n",
        "    if os.path.exists(classes_subdir):\n",
        "        import shutil\n",
        "        for f in os.listdir(classes_subdir):\n",
        "            shutil.move(os.path.join(classes_subdir, f), os.path.join(ANN_DIR, f))\n",
        "        shutil.rmtree(classes_subdir)\n",
        "    print(\"   ‚úÖ Extracted class files\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import google.auth  # <--- FIXED: Added missing import\n",
        "from google.colab import auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2 import credentials\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION\n",
        "# ==============================================================================\n",
        "FILE_ID = \"1GuRdUMP5qrqyYN0gg8C2B6tLwJeigyFd\"  # Your 154GB File ID\n",
        "LOCAL_FILE = \"/content/frames.tar\"\n",
        "MOUNT_POINT = \"/content/yowo/data/ActionGenome/frames\"\n",
        "# ==============================================================================\n",
        "\n",
        "def install_tools():\n",
        "    print(\"üõ†Ô∏è Installing aria2 and ratarmount...\")\n",
        "    # -qq suppresses the wall of text\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"-qq\", \"aria2\"], check=True)\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"ratarmount\"], check=True)\n",
        "\n",
        "def get_token():\n",
        "    print(\"üîë Authenticating (to bypass Quota limits)...\")\n",
        "    auth.authenticate_user()\n",
        "    # Get the raw token string to pass to aria2c\n",
        "    creds, _ = google.auth.default()\n",
        "    creds.refresh(Request())\n",
        "    return creds.token\n",
        "\n",
        "def download_fast(token):\n",
        "    if os.path.exists(LOCAL_FILE):\n",
        "        print(f\"‚úÖ File already exists at {LOCAL_FILE}. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüöÄ STARTING MULTI-THREADED DOWNLOAD (16x Streams)...\")\n",
        "    print(\"   This uses aria2c with your auth token. Max speed.\")\n",
        "    \n",
        "    # URL for Drive API download\n",
        "    url = f\"https://www.googleapis.com/drive/v3/files/{FILE_ID}?alt=media\"\n",
        "    \n",
        "    # aria2c command:\n",
        "    # -x 16: 16 connections\n",
        "    # -s 16: Split file into 16 parts\n",
        "    # -j 16: Max concurrent downloads\n",
        "    # --header: Pass the OAuth token\n",
        "    cmd = [\n",
        "        \"aria2c\", \n",
        "        \"-x\", \"16\", \n",
        "        \"-s\", \"16\", \n",
        "        \"-j\", \"16\",\n",
        "        \"--file-allocation=none\", \n",
        "        \"--summary-interval=10\",\n",
        "        \"--header\", f\"Authorization: Bearer {token}\", \n",
        "        \"-o\", os.path.basename(LOCAL_FILE),\n",
        "        \"-d\", os.path.dirname(LOCAL_FILE),\n",
        "        url\n",
        "    ]\n",
        "    \n",
        "    # Stream output to console\n",
        "    process = subprocess.Popen(cmd)\n",
        "    process.wait()\n",
        "    \n",
        "    if process.returncode != 0:\n",
        "        raise Exception(\"Download failed! Check quota or network.\")\n",
        "    print(\"\\n‚úÖ Download Complete.\")\n",
        "\n",
        "def mount_archive():\n",
        "    print(f\"\\nüîó Mounting {LOCAL_FILE} to {MOUNT_POINT}...\")\n",
        "    \n",
        "    # Clean up previous mount\n",
        "    subprocess.run([\"fusermount\", \"-u\", MOUNT_POINT], stderr=subprocess.DEVNULL)\n",
        "    os.makedirs(MOUNT_POINT, exist_ok=True)\n",
        "    \n",
        "    # Mount with 4 threads for read speed\n",
        "    index_file = LOCAL_FILE + \".index.sqlite\"\n",
        "    cmd = f\"ratarmount -P 4 --index-file '{index_file}' '{LOCAL_FILE}' '{MOUNT_POINT}'\"\n",
        "    os.system(cmd)\n",
        "    \n",
        "    # Verify\n",
        "    if len(os.listdir(MOUNT_POINT)) > 0:\n",
        "        print(f\"üéâ SUCCESS! {len(os.listdir(MOUNT_POINT))} items visible.\")\n",
        "        print(f\"üëâ Data is ready at: {MOUNT_POINT}\")\n",
        "    else:\n",
        "        print(\"‚ùå Mount failed. Folder is empty.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "try:\n",
        "    # 1. Clear any existing Drive mounts\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.flush_and_unmount()\n",
        "    except: pass\n",
        "    \n",
        "    install_tools()\n",
        "    \n",
        "    # 2. Get Token & Download\n",
        "    token = get_token()\n",
        "    download_fast(token)\n",
        "    \n",
        "    # 3. Mount\n",
        "    mount_archive()\n",
        "    \n",
        "    # 4. Final Space Check\n",
        "    print(\"\\nüìä Storage Status:\")\n",
        "    os.system(\"df -h /content\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå CRITICAL ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Verify Dataset Structure\n",
        "import os, pickle\n",
        "\n",
        "ANN_DIR = \"/content/yowo/data/ActionGenome/annotations\"\n",
        "FRAMES_DIR = \"/content/yowo/data/ActionGenome/frames\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîç Dataset Verification\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check required files\n",
        "required_files = {\n",
        "    'person_bbox.pkl': 'Person bounding boxes + keyframes',\n",
        "    'object_bbox_and_relationship.pkl': 'Objects + relationships',\n",
        "    'Charades_v1_train.csv': 'Training action labels',\n",
        "    'Charades_v1_test.csv': 'Test action labels',\n",
        "    'Charades_v1_classes.txt': '157 action classes',\n",
        "    'object_classes.txt': '36 object classes',\n",
        "    'relationship_classes.txt': '26 relationship classes',\n",
        "    'video_fps.json': 'FPS for each video'\n",
        "}\n",
        "\n",
        "print(\"\\nüìã Required Annotation Files:\")\n",
        "all_ok = True\n",
        "for f, desc in required_files.items():\n",
        "    path = os.path.join(ANN_DIR, f)\n",
        "    if os.path.exists(path):\n",
        "        size = os.path.getsize(path) / 1e6\n",
        "        print(f\"   ‚úÖ {f} ({size:.1f} MB) - {desc}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {f} - MISSING! ({desc})\")\n",
        "        all_ok = False\n",
        "\n",
        "# Check frames\n",
        "print(f\"\\nüìÇ Frames Directory:\")\n",
        "if os.path.exists(FRAMES_DIR):\n",
        "    num_videos = len(os.listdir(FRAMES_DIR))\n",
        "    print(f\"   ‚úÖ {num_videos} video directories\")\n",
        "    # Sample a video\n",
        "    sample_vid = os.listdir(FRAMES_DIR)[0]\n",
        "    sample_frames = len(os.listdir(os.path.join(FRAMES_DIR, sample_vid)))\n",
        "    print(f\"   üìÅ Sample: {sample_vid} has {sample_frames} frames\")\n",
        "else:\n",
        "    print(\"   ‚ùå Frames directory missing!\")\n",
        "    all_ok = False\n",
        "\n",
        "# Verify PKL files are valid\n",
        "print(f\"\\nüî¨ Validating PKL Files:\")\n",
        "try:\n",
        "    with open(os.path.join(ANN_DIR, 'person_bbox.pkl'), 'rb') as f:\n",
        "        person_data = pickle.load(f)\n",
        "    print(f\"   ‚úÖ person_bbox.pkl: {len(person_data)} keyframes\")\n",
        "    \n",
        "    with open(os.path.join(ANN_DIR, 'object_bbox_and_relationship.pkl'), 'rb') as f:\n",
        "        obj_data = pickle.load(f)\n",
        "    print(f\"   ‚úÖ object_bbox_and_relationship.pkl: {len(obj_data)} entries\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error reading PKL files: {e}\")\n",
        "    all_ok = False\n",
        "\n",
        "if all_ok:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚úÖ DATASET READY FOR TRAINING!\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"‚ö†Ô∏è DATASET INCOMPLETE - Check errors above\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Understanding Training Output\n",
        "# Training progress is shown via console output every 10 iterations\n",
        "# For the multi-task model, you'll see:\n",
        "#\n",
        "# [Epoch: 1/10][Iter: 100/288782][lr: 0.0001]\n",
        "# [loss_conf: 8.64][loss_obj: 3.56][loss_act: 24.52][loss_rel: 17.26]\n",
        "# [loss_interact: 4.76][loss_box: 1.04][losses: 63.97][time: 4.71]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä MULTI-TASK TRAINING OUTPUT GUIDE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "The multi-task model outputs 6 separate losses:\n",
        "\n",
        "| Loss | Description | Typical Start | Target |\n",
        "|------|-------------|---------------|--------|\n",
        "| loss_conf | Confidence/objectness | ~10-15 | ~3-5 |\n",
        "| loss_obj | Object classification (36 classes) | ~3-4 | ~1-2 |\n",
        "| loss_act | Action classification (157 classes) | ~15-25 | ~5-10 |\n",
        "| loss_rel | Relationship classification (26 classes) | ~15-20 | ~3-5 |\n",
        "| loss_interact | Interaction detection | ~4-5 | ~1-2 |\n",
        "| loss_box | Bounding box regression | ~0.5-1.0 | ~0.3-0.5 |\n",
        "| losses | TOTAL (sum of above) | ~50-70 | ~15-25 |\n",
        "\n",
        "Note: loss_act may be 0.00 for some batches - this is NORMAL!\n",
        "      Actions only apply to Person boxes, and some frames have no person.\n",
        "\n",
        "Training time estimates (with AMP):\n",
        "- T4 (Colab): bs=8, ~0.4 sec/iter, ~4 hours/epoch\n",
        "- L4: bs=12, ~0.25 sec/iter, ~2.5 hours/epoch\n",
        "- A100-40GB: bs=32, ~0.1 sec/iter, ~50 min/epoch\n",
        "- A100-80GB: bs=64, ~0.05 sec/iter, ~30 min/epoch\n",
        "\"\"\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Ready to Train!\n",
        "\n",
        "**Model Architecture: `yowo_v2_x3d_m_yolo11m_multitask`**\n",
        "\n",
        "| Component | Description |\n",
        "|-----------|-------------|\n",
        "| 2D Backbone | YOLO11m (pretrained on COCO) |\n",
        "| 3D Backbone | X3D-M (pretrained on Kinetics-400) |\n",
        "| Object Head | 36 classes (person + 35 objects) |\n",
        "| Action Head | 157 Charades action classes |\n",
        "| Relation Head | 26 relationship classes |\n",
        "| Interaction Head | Binary (is object interacted with?) |\n",
        "\n",
        "**Dataset: Charades + Action Genome**\n",
        "- 288,782 annotated keyframes\n",
        "- 9,601 videos\n",
        "- Multi-task: Objects + Actions + Relationships\n",
        "\n",
        "**Note:** Model checkpoints saved after each epoch to `/content/yowo/weights/charades_ag/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: üöÄ TRAIN! (Main training cell)\n",
        "# Batch size and accumulation are auto-configured from Cell 1\n",
        "# AMP (Automatic Mixed Precision) enabled for ~1.5-2x faster training!\n",
        "\n",
        "import os\n",
        "os.chdir('/content/yowo')\n",
        "\n",
        "# Training configuration\n",
        "MAX_EPOCHS = 10          # Number of epochs (1 epoch = 288,782 iterations at bs=1)\n",
        "LEARNING_RATE = 0.0001   # Base learning rate\n",
        "LEN_CLIP = 16            # Number of frames per clip (temporal window)\n",
        "NUM_WORKERS = 2          # DataLoader workers\n",
        "\n",
        "# Build command with auto-configured batch size + AMP\n",
        "cmd = f\"\"\"python train.py \\\n",
        "    -d charades_ag \\\n",
        "    -v yowo_v2_x3d_m_yolo11m_multitask \\\n",
        "    --cuda \\\n",
        "    --amp \\\n",
        "    -bs {BATCH_SIZE} \\\n",
        "    -accu {ACCUMULATE} \\\n",
        "    --max_epoch {MAX_EPOCHS} \\\n",
        "    --root /content/yowo/data \\\n",
        "    -K {LEN_CLIP} \\\n",
        "    -lr {LEARNING_RATE} \\\n",
        "    --num_workers {NUM_WORKERS} \\\n",
        "    --save_folder /content/yowo/weights\"\"\"\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üì¶ Batch size: {BATCH_SIZE} (effective: {BATCH_SIZE*ACCUMULATE})\")\n",
        "print(f\"üìä Epochs: {MAX_EPOCHS}\")\n",
        "print(f\"üìà Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"üé¨ Clip length: {LEN_CLIP} frames\")\n",
        "print(f\"‚ö° AMP: Enabled\")\n",
        "print(f\"\\nüìã Full command:\\n{cmd}\\n\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "!{cmd}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Save Weights to Google Drive (after training)\n",
        "import shutil, os\n",
        "\n",
        "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/yooowo/weights\"\n",
        "os.makedirs(DRIVE_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "weights_dir = \"/content/yowo/weights/charades_ag/yowo_v2_x3d_m_yolo11m_multitask\"\n",
        "if os.path.exists(weights_dir):\n",
        "    for w in os.listdir(weights_dir):\n",
        "        if w.endswith('.pth'):\n",
        "            shutil.copy2(os.path.join(weights_dir, w), os.path.join(DRIVE_SAVE_PATH, w))\n",
        "            print(f\"‚úÖ Saved {w} to Drive\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No weights found yet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Optional: Quick 1-Epoch Test\n",
        "\n",
        "Run this first to verify everything works before full training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test - run ~100 iterations to verify everything works\n",
        "# Uses small batch to ensure it fits, includes AMP\n",
        "# Uncomment the line below to run:\n",
        "\n",
        "# !python train.py -d charades_ag -v yowo_v2_x3d_m_yolo11m_multitask --cuda --amp -bs 4 --max_epoch 1 --root /content/yowo/data -K 16 --num_workers 2 2>&1 | head -80\n",
        "\n",
        "# If it works, you should see losses decreasing every 10 iterations.\n",
        "# Then run Cell 8 for full training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Resume Training from Checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resume from checkpoint (uncomment and modify path)\n",
        "# CHECKPOINT = \"/content/yowo/weights/charades_ag/yowo_v2_x3d_m_yolo11m_multitask/yowo_v2_x3d_m_yolo11m_multitask_epoch_5.pth\"\n",
        "# !python train.py -d charades_ag -v yowo_v2_x3d_m_yolo11m_multitask --cuda -bs {BATCH_SIZE} -accu {ACCUMULATE} --max_epoch 20 --root /content/yowo/data -K 16 -r {CHECKPOINT} --eval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Troubleshooting\n",
        "\n",
        "| Problem | Solution |\n",
        "|---------|----------|\n",
        "| **OOM Error** | Reduce `BATCH_SIZE` by 2, increase `ACCUMULATE` proportionally (keep effective same) |\n",
        "| **Training slow** | Increase batch size if GPU memory allows. L4/A100 can go higher. |\n",
        "| **Loss not decreasing** | Try lr=0.0005 (higher) or lr=0.00005 (lower) |\n",
        "| **`loss is NAN !!`** | Reduce learning rate to 0.00005, or check for bad data samples |\n",
        "| **Loss stuck high** | Verify dataset extracted correctly, check annotations |\n",
        "| **loss_act = 0.00** | This is NORMAL - some frames have no person, so no action loss |\n",
        "\n",
        "## üìÅ Output Files\n",
        "\n",
        "After training:\n",
        "- **Weights**: `/content/yowo/weights/charades_ag/yowo_v2_x3d_m_yolo11m_multitask/`\n",
        "- **Checkpoints**: `yowo_v2_x3d_m_yolo11m_multitask_epoch_N.pth`\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT:** Run Cell 9 to copy weights to Google Drive before the runtime disconnects!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
